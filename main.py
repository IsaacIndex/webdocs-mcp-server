from fastmcp import FastMCP
from typing import Dict, Any, List, Optional
import logging
import argparse
from selenium import webdriver
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup, Tag
from langdetect import detect, LangDetectException
import re
import time
import os
from urllib.parse import urljoin
import sys
import requests
import shutil

# Configure logging
parser = argparse.ArgumentParser(description="Web Scraper MCP Server")
parser.add_argument(
    "--log-level",
    default="INFO",
    help="Logging level (debug, info, warning, error, critical)"
)
args, _ = parser.parse_known_args()

log_file = os.path.join(os.path.expanduser("~"), "Downloads", "mcp.log")
log_level = getattr(logging, args.log_level.upper(), logging.INFO)
logging.basicConfig(
    level=log_level,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file)
    ]
)
logger = logging.getLogger(__name__)

# Initialize Chrome options
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--window-size=1920,1080')
user_agent = (
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
    'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
)
chrome_options.add_argument(f'--user-agent={user_agent}')

# Initialize FastMCP
mcp = FastMCP("Web Scraper MCP ðŸš€")


def _get_chrome_profile_path() -> str:
    """Return the default Chrome user profile path based on the OS."""
    home = os.path.expanduser("~")
    if sys.platform == "darwin":
        return os.path.join(home, "Library", "Application Support", "Google", "Chrome")
    if sys.platform.startswith("linux"):
        return os.path.join(home, ".config", "google-chrome")
    if sys.platform.startswith("win"):
        return os.path.join(home, "AppData", "Local", "Google", "Chrome", "User Data")
    raise RuntimeError("Unsupported operating system for locating Chrome profile")


def _get_chrome_binary() -> Optional[str]:
    """Return the Chrome executable path or None if not found."""
    env_binary = os.getenv("CHROME_BINARY")
    if env_binary and os.path.exists(env_binary):
        return env_binary

    candidates = [
        shutil.which("google-chrome"),
        shutil.which("chromium-browser"),
        shutil.which("chrome"),
        "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
    ]

    for path in candidates:
        if path and os.path.exists(path):
            return path
    return None


class WebScraper:
    def __init__(self) -> None:
        self.driver: Optional[WebDriver] = None
        # Elements to remove from the content
        self.unwanted_elements = [
            'script', 'style', 'nav', 'footer', 'header', 'aside',
            'iframe', 'noscript', 'svg', 'form', 'button', 'input',
            'meta', 'link', 'img', 'video', 'audio'
        ]
        self.unwanted_classes = [
            'menu', 'navigation', 'sidebar', 'footer', 'header',
            'advertisement', 'banner', 'cookie', 'popup', 'modal',
            'comment', 'social', 'share', 'related', 'recommended'
        ]
        self.unwanted_class_patterns = [re.compile(cls, re.IGNORECASE) for cls in self.unwanted_classes]
        self.non_content_patterns = [
            re.compile(r'^\s*$'),
            re.compile(r'^[0-9\s]+$'),
            re.compile(r'^[A-Z\s]+$'),
            re.compile(r'cookie|privacy|terms|conditions', re.IGNORECASE)
        ]
        try:
            binary = _get_chrome_binary()
            if binary:
                chrome_options.binary_location = binary
                logger.info(f"Using Chrome binary at {binary}")
            else:
                logger.warning("Chrome binary not found; relying on default discovery")

            logger.info("Initializing Chrome WebDriver...")
            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()),
                options=chrome_options,
            )
            logger.info("Chrome WebDriver initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Chrome WebDriver: {str(e)}")
            self.driver = None

    def _ensure_driver(self) -> None:
        if self.driver:
            return
        try:
            logger.info("Attempting to reinitialize Chrome WebDriver...")
            binary = _get_chrome_binary()
            if binary:
                chrome_options.binary_location = binary
                logger.info(f"Using Chrome binary at {binary}")
            else:
                logger.warning("Chrome binary not found; relying on default discovery")

            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()),
                options=chrome_options,
            )
            logger.info("Chrome WebDriver reinitialized successfully")
        except Exception as e:
            error_msg = f"Failed to initialize Chrome WebDriver: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)

    def _clean_text(self, text: str) -> str:
        """Clean and normalize text content."""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters but keep basic punctuation
        text = re.sub(r'[^\w\s.,!?-]', '', text)
        # Remove multiple punctuation
        text = re.sub(r'([.,!?-])\1+', r'\1', text)
        # Remove leading/trailing whitespace
        text = text.strip()
        return text

    def _is_relevant_content(self, text: str) -> bool:
        """Check if the text content is relevant."""
        # Minimum content length
        if len(text) < 20:
            return False
        for pattern in self.non_content_patterns:
            if pattern.search(text):
                return False
        return True

    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """Extract the main content from the page."""
        # Remove unwanted elements
        for element in self.unwanted_elements:
            for tag in soup.find_all(element):
                tag.decompose()

        # Remove elements with unwanted classes
        for class_pattern in self.unwanted_class_patterns:
            for tag in soup.find_all(class_=class_pattern):
                tag.decompose()

        # Try to find the main content area
        main_content = None
        for tag in ['article', 'main', 'div[role="main"]', '.content', '#content']:
            main_content = soup.select_one(tag)
            if main_content:
                break

        # If no main content found, use the body
        if not main_content:
            main_content = soup.body

        return main_content.get_text(separator='\n', strip=True) if main_content else ''

    def extract_links(self, url: str) -> List[Dict[str, str]]:
        """Extract all links from the webpage."""
        self._ensure_driver()
        assert self.driver is not None

        try:
            logger.info(f"Extracting links from URL: {url}")
            self.driver.get(url)
            WebDriverWait(self.driver, 30).until(
                EC.presence_of_element_located(("tag name", "body"))
            )
            time.sleep(2)  # Shorter wait time for links

            # Get the page source and parse with BeautifulSoup
            html_content = self.driver.page_source
            soup = BeautifulSoup(html_content, 'html.parser')

            # Extract all links
            links = []
            for a_tag in soup.find_all('a', href=True):
                # Ensure the element is a Tag before accessing attributes
                if not isinstance(a_tag, Tag):
                    continue

                href_value = a_tag.get('href', '')
                if isinstance(href_value, list):
                    href = href_value[0] if href_value else ""
                else:
                    href = href_value or ""

                text = a_tag.get_text(strip=True)

                # Skip empty links and javascript:void(0)
                if not href or href.startswith('javascript:'):
                    continue

                # Make relative URLs absolute
                if not href.startswith(('http://', 'https://')):
                    href = urljoin(url, href)

                links.append({
                    "url": href,
                    "text": text if text else href
                })

            logger.info(f"Successfully extracted {len(links)} links from {url}")
            return links

        except Exception as e:
            error_msg = f"Error extracting links from {url}: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)

    def fetch_content(self, url: str) -> str:
        self._ensure_driver()
        assert self.driver is not None

        try:
            logger.info(f"Fetching content from URL: {url}")
            self.driver.get(url)
            WebDriverWait(self.driver, 30).until(
                EC.presence_of_element_located(("tag name", "body"))
            )
            time.sleep(5)

            # Get the page source and parse with BeautifulSoup
            html_content = self.driver.page_source
            soup = BeautifulSoup(html_content, 'html.parser')

            # Extract main content
            text = self._extract_main_content(soup)

            # Clean and process the text
            lines = text.split('\n')
            cleaned_lines = []

            for line in lines:
                cleaned_line = self._clean_text(line)
                if self._is_relevant_content(cleaned_line):
                    cleaned_lines.append(cleaned_line)

            # Join lines and remove duplicate content
            text = '\n'.join(cleaned_lines)
            text = re.sub(r'\n\s*\n', '\n\n', text)  # Remove multiple newlines

            # Optional: Detect language and filter non-English content
            try:
                if detect(text) != 'en':
                    logger.warning(f"Non-English content detected from {url}")
            except LangDetectException:
                logger.warning(f"Could not detect language for content from {url}")

            logger.info(f"Successfully extracted and cleaned text content from {url}")
            return text

        except Exception as e:
            error_msg = f"Error fetching content from {url}: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)

    def cleanup(self) -> None:
        if self.driver:
            try:
                self.driver.quit()
                logger.info("Chrome WebDriver cleaned up successfully")
            except Exception as e:
                logger.error(f"Error during WebDriver cleanup: {str(e)}")


@mcp.tool
def open_in_user_browser(url: str) -> Dict[str, Any]:
    """Open a URL in the user's regular Chrome profile."""
    try:
        profile = _get_chrome_profile_path()
        options = Options()
        options.add_argument(f"--user-data-dir={profile}")
        driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()), options=options
        )
        driver.get(url)
        page_source = driver.page_source
        return {
            "status": "success",
            "message": f"Opened {url} in the user browser",
            "data": page_source,
        }
    except Exception as e:
        logger.error(f"Failed to open browser for {url}: {str(e)}")
        return {"status": "error", "message": str(e), "data": None}


# Initialize WebScraper
scraper = WebScraper()


@mcp.tool
def scrape_website(url: str) -> Dict[str, Any]:
    """Scrape content from a specified URL"""
    try:
        content = scraper.fetch_content(url)
        return {
            "status": "success",
            "no. of characters": len(content),
            "message": "Website content scraped successfully",
            "data": {"content": content}
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "data": None
        }


@mcp.tool
def extract_links(url: str) -> Dict[str, Any]:
    """Extract all links from a specified URL"""
    try:
        links = scraper.extract_links(url)
        return {
            "status": "success",
            "no. of links": len(links),
            "message": "Links extracted successfully",
            "data": {"links": links}
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "data": None
        }


@mcp.tool
def download_pdfs_from_text(text: str) -> Dict[str, Any]:
    """Download all PDF links found in the provided text."""
    try:
        pdf_pattern = re.compile(r"https?://[^\s'\"<>]+?\.pdf", re.IGNORECASE)
        links = pdf_pattern.findall(text)
        download_dir = os.path.join(os.path.expanduser("~"), "Downloads")
        os.makedirs(download_dir, exist_ok=True)
        downloaded_files: List[str] = []

        for link in links:
            # Remove trailing punctuation
            clean_link = link.rstrip(').,')
            file_name = os.path.basename(clean_link.split("?")[0])
            file_path = os.path.join(download_dir, file_name)
            logger.info(f"Downloading PDF from {clean_link} to {file_path}")
            response = requests.get(clean_link, timeout=30)
            response.raise_for_status()
            with open(file_path, "wb") as pdf_file:
                pdf_file.write(response.content)
            downloaded_files.append(file_path)

        return {
            "status": "success",
            "no. of files": len(downloaded_files),
            "message": "PDF files downloaded" if downloaded_files else "No PDF links found",
            "data": {"files": downloaded_files},
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "data": None,
        }


@mcp.tool
def ping() -> Dict[str, Any]:
    """Check if the server is responsive"""
    return {
        "status": "success",
        "message": "Pong",
        "data": {"timestamp": time.time()}
    }


if __name__ == "__main__":
    try:
        mcp.run()
    finally:
        scraper.cleanup()
